{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maocafe29/cs_9414/blob/main/ass2_9414.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyzaj1OKSHpJ"
      },
      "source": [
        "# COMP9414 Artificial Intelligence - Assignment 2: Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqwZWH-5SHpM"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9NFxs9HSHpN"
      },
      "outputs": [],
      "source": [
        "# Check if running in Google Colab\n",
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag-tT8m3SHpO"
      },
      "outputs": [],
      "source": [
        "# File setup for Colab (skip if running locally)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs('images', exist_ok=True)\n",
        "\n",
        "    # Function to check and upload files\n",
        "    def setup_files():\n",
        "        required_files = {\n",
        "            'env.py': '.',\n",
        "            'grid_agent.png': 'images',\n",
        "            'grid_goal_position.png': 'images',\n",
        "            'grid_obstacle.png': 'images'\n",
        "        }\n",
        "\n",
        "        for filename, directory in required_files.items():\n",
        "            filepath = os.path.join(directory, filename) if directory != '.' else filename\n",
        "            if not os.path.exists(filepath):\n",
        "                print(f\"Please upload {filename}:\")\n",
        "                uploaded = files.upload()\n",
        "                if filename in uploaded and directory != '.':\n",
        "                    os.rename(filename, filepath)\n",
        "\n",
        "    setup_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ6xsMwCSHpP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from env import GridWorldEnv\n",
        "\n",
        "# Set up matplotlib\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxLLyyZISHpQ"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "All tasks will use the same hyperparameters to ensure fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aNgYOBZSHpQ"
      },
      "outputs": [],
      "source": [
        "# Global hyperparameters (same for all tasks)\n",
        "HYPERPARAMETERS = {\n",
        "    'learning_rate': 0.3,\n",
        "    'discount_factor': 0.95,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay_episodes': 300,\n",
        "    'num_episodes': 500,\n",
        "    'max_steps_per_episode': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Teacher parameters (as per assignment requirements)\n",
        "TEACHER_PARAMS = {\n",
        "    'availability': [0.1, 0.3, 0.5, 0.7, 1.0],\n",
        "    'accuracy': [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "}\n",
        "\n",
        "print(\"Hyperparameters:\")\n",
        "for key, value in HYPERPARAMETERS.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAj7OSf3SHpR"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqp6Q-DeSHpR"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(episode_rewards, episode_steps, successful_episodes):\n",
        "    \"\"\"Calculate performance metrics\"\"\"\n",
        "    success_rate = (sum(successful_episodes) / len(successful_episodes)) * 100\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "    avg_steps = np.mean(episode_steps)\n",
        "    avg_learning_speed = 1 / avg_steps if avg_steps > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'success_rate': success_rate,\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_learning_speed': avg_learning_speed\n",
        "    }\n",
        "\n",
        "def plot_learning_curve(rewards, title, window=50):\n",
        "    \"\"\"Plot learning curve with moving average\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot raw rewards with transparency\n",
        "    plt.plot(rewards, alpha=0.3, color='blue', label='Episode Rewards')\n",
        "\n",
        "    # Calculate and plot moving average\n",
        "    if len(rewards) >= window:\n",
        "        moving_avg = pd.Series(rewards).rolling(window=window).mean()\n",
        "        plt.plot(moving_avg, color='darkblue', linewidth=2,\n",
        "                label=f'{window}-episode MA')\n",
        "\n",
        "    # Add reference line at y=0\n",
        "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward per Episode')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def create_heatmap(results_df, algorithm_name):\n",
        "    \"\"\"Create heatmap for teacher experiment results\"\"\"\n",
        "    # Pivot data for heatmap\n",
        "    pivot_table = results_df.pivot(index='accuracy',\n",
        "                                  columns='availability',\n",
        "                                  values='avg_reward')\n",
        "\n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(pivot_table, annot=True, fmt='.1f', cmap='RdYlBu_r',\n",
        "                cbar_kws={'label': 'Average Reward'})\n",
        "    plt.title(f'{algorithm_name} with Teacher - Average Reward Heatmap')\n",
        "    plt.xlabel('Teacher Availability')\n",
        "    plt.ylabel('Teacher Accuracy')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_9DOIaBSHpS"
      },
      "source": [
        "# Task 1: Implement Q-learning\n",
        "\n",
        "In this task, we implement the Q-learning algorithm and train an agent in the grid world environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayLeNeHOSHpT"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"Q-learning agent implementation\"\"\"\n",
        "\n",
        "    def __init__(self, env, learning_rate=0.3, discount_factor=0.95,\n",
        "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_episodes=300):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay_episodes = epsilon_decay_episodes\n",
        "\n",
        "        # Initialize Q-table\n",
        "        self.q_table = np.zeros((env.grid_size, env.grid_size, env.action_space))\n",
        "\n",
        "    def get_epsilon(self, episode):\n",
        "        \"\"\"Calculate epsilon value for current episode (linear decay)\"\"\"\n",
        "        if episode >= self.epsilon_decay_episodes:\n",
        "            return self.epsilon_end\n",
        "        return self.epsilon_start - (self.epsilon_start - self.epsilon_end) * \\\n",
        "               (episode / self.epsilon_decay_episodes)\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            return np.random.randint(self.env.action_space)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state[0], state[1]])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update rule\"\"\"\n",
        "        current_q = self.q_table[state[0], state[1], action]\n",
        "\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Q-learning: use maximum Q-value of next state\n",
        "            max_next_q = np.max(self.q_table[next_state[0], next_state[1]])\n",
        "            target = reward + self.discount_factor * max_next_q\n",
        "\n",
        "        # Update Q-value\n",
        "        self.q_table[state[0], state[1], action] = \\\n",
        "            current_q + self.learning_rate * (target - current_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8J7PLEUSHpU"
      },
      "outputs": [],
      "source": [
        "def train_q_learning(env, agent, num_episodes):\n",
        "    \"\"\"Train Q-learning agent\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    successful_episodes = []\n",
        "\n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training Q-learning\"):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        epsilon = agent.get_epsilon(episode)\n",
        "\n",
        "        while not done and steps < HYPERPARAMETERS['max_steps_per_episode']:\n",
        "            # Choose action\n",
        "            action = agent.choose_action(state, epsilon)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            # Update state and statistics\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        # Record metrics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_steps.append(steps)\n",
        "        successful_episodes.append(1 if done and reward == 25 else 0)\n",
        "\n",
        "    return episode_rewards, episode_steps, successful_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkf1lavQSHpU"
      },
      "outputs": [],
      "source": [
        "# Create environment\n",
        "env = GridWorldEnv(seed=HYPERPARAMETERS['seed'])\n",
        "\n",
        "# Create and train Q-learning agent\n",
        "print(\"Training Q-learning agent...\")\n",
        "q_agent = QLearningAgent(env, **{k: v for k, v in HYPERPARAMETERS.items()\n",
        "                                if k not in ['num_episodes', 'max_steps_per_episode', 'seed']})\n",
        "\n",
        "q_rewards, q_steps, q_success = train_q_learning(env, q_agent, HYPERPARAMETERS['num_episodes'])\n",
        "\n",
        "# Calculate metrics\n",
        "q_metrics = calculate_metrics(q_rewards, q_steps, q_success)\n",
        "\n",
        "print(\"\\nQ-learning Results:\")\n",
        "print(f\"Success Rate: {q_metrics['success_rate']:.2f}%\")\n",
        "print(f\"Average Reward: {q_metrics['avg_reward']:.2f}\")\n",
        "print(f\"Average Learning Speed: {q_metrics['avg_learning_speed']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8RgetsCSHpU"
      },
      "outputs": [],
      "source": [
        "# Plot Q-learning performance\n",
        "plot_learning_curve(q_rewards, \"Q-learning Performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxENWp_SHpV"
      },
      "source": [
        "# Task 2: Implement SARSA\n",
        "\n",
        "In this task, we implement the SARSA algorithm using the same hyperparameters as Task 1 for fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94MGkIKSSHpW"
      },
      "outputs": [],
      "source": [
        "class SARSAAgent:\n",
        "    \"\"\"SARSA agent implementation\"\"\"\n",
        "\n",
        "    def __init__(self, env, learning_rate=0.3, discount_factor=0.95,\n",
        "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_episodes=300):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay_episodes = epsilon_decay_episodes\n",
        "\n",
        "        # Initialize Q-table\n",
        "        self.q_table = np.zeros((env.grid_size, env.grid_size, env.action_space))\n",
        "\n",
        "    def get_epsilon(self, episode):\n",
        "        \"\"\"Calculate epsilon value for current episode (linear decay)\"\"\"\n",
        "        if episode >= self.epsilon_decay_episodes:\n",
        "            return self.epsilon_end\n",
        "        return self.epsilon_start - (self.epsilon_start - self.epsilon_end) * \\\n",
        "               (episode / self.epsilon_decay_episodes)\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            return np.random.randint(self.env.action_space)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state[0], state[1]])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_action, done):\n",
        "        \"\"\"SARSA update rule\"\"\"\n",
        "        current_q = self.q_table[state[0], state[1], action]\n",
        "\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # SARSA: use Q-value of next state-action pair\n",
        "            next_q = self.q_table[next_state[0], next_state[1], next_action]\n",
        "            target = reward + self.discount_factor * next_q\n",
        "\n",
        "        # Update Q-value\n",
        "        self.q_table[state[0], state[1], action] = \\\n",
        "            current_q + self.learning_rate * (target - current_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am8ay6ktSHpW"
      },
      "outputs": [],
      "source": [
        "def train_sarsa(env, agent, num_episodes):\n",
        "    \"\"\"Train SARSA agent\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    successful_episodes = []\n",
        "\n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training SARSA\"):\n",
        "        state = env.reset()\n",
        "        epsilon = agent.get_epsilon(episode)\n",
        "        action = agent.choose_action(state, epsilon)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < HYPERPARAMETERS['max_steps_per_episode']:\n",
        "            # Take action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Choose next action\n",
        "            next_action = agent.choose_action(next_state, epsilon)\n",
        "\n",
        "            # Update Q-value\n",
        "            agent.update(state, action, reward, next_state, next_action, done)\n",
        "\n",
        "            # Update state and action\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        # Record metrics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_steps.append(steps)\n",
        "        successful_episodes.append(1 if done and reward == 25 else 0)\n",
        "\n",
        "    return episode_rewards, episode_steps, successful_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0drGmri_SHpX"
      },
      "outputs": [],
      "source": [
        "# Create and train SARSA agent\n",
        "print(\"Training SARSA agent...\")\n",
        "sarsa_agent = SARSAAgent(env, **{k: v for k, v in HYPERPARAMETERS.items()\n",
        "                                if k not in ['num_episodes', 'max_steps_per_episode', 'seed']})\n",
        "\n",
        "sarsa_rewards, sarsa_steps, sarsa_success = train_sarsa(env, sarsa_agent, HYPERPARAMETERS['num_episodes'])\n",
        "\n",
        "# Calculate metrics\n",
        "sarsa_metrics = calculate_metrics(sarsa_rewards, sarsa_steps, sarsa_success)\n",
        "\n",
        "print(\"\\nSARSA Results:\")\n",
        "print(f\"Success Rate: {sarsa_metrics['success_rate']:.2f}%\")\n",
        "print(f\"Average Reward: {sarsa_metrics['avg_reward']:.2f}\")\n",
        "print(f\"Average Learning Speed: {sarsa_metrics['avg_learning_speed']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbv6KIn4SHpX"
      },
      "outputs": [],
      "source": [
        "# Plot SARSA performance\n",
        "plot_learning_curve(sarsa_rewards, \"SARSA Performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18UpDGxSHpX"
      },
      "source": [
        "## Baseline Comparison\n",
        "\n",
        "Compare the performance of Q-learning and SARSA without teacher guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV4wbx-OSHpX"
      },
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Learning progress comparison\n",
        "ax1.plot(q_rewards, alpha=0.3, color='blue', label='Q-learning')\n",
        "ax1.plot(sarsa_rewards, alpha=0.3, color='green', label='SARSA')\n",
        "\n",
        "# Add moving averages\n",
        "window = 50\n",
        "if len(q_rewards) >= window:\n",
        "    q_ma = pd.Series(q_rewards).rolling(window=window).mean()\n",
        "    sarsa_ma = pd.Series(sarsa_rewards).rolling(window=window).mean()\n",
        "    ax1.plot(q_ma, color='darkblue', linewidth=2, label=f'Q-learning MA({window})')\n",
        "    ax1.plot(sarsa_ma, color='darkgreen', linewidth=2, label=f'SARSA MA({window})')\n",
        "\n",
        "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Reward per Episode')\n",
        "ax1.set_title('Learning Progress Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Success rate comparison\n",
        "window = 50\n",
        "q_success_rate = pd.Series(q_success).rolling(window=window).mean() * 100\n",
        "sarsa_success_rate = pd.Series(sarsa_success).rolling(window=window).mean() * 100\n",
        "\n",
        "ax2.plot(q_success_rate, color='blue', label=f'Q-learning ({q_metrics[\"success_rate\"]:.1f}% overall)')\n",
        "ax2.plot(sarsa_success_rate, color='green', label=f'SARSA ({sarsa_metrics[\"success_rate\"]:.1f}% overall)')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Success Rate (%)')\n",
        "ax2.set_title('Success Rate Comparison')\n",
        "ax2.set_ylim(0, 105)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPqCtbzSHpY"
      },
      "source": [
        "# Task 3: Q-learning with Teacher\n",
        "\n",
        "Implement teacher-student framework where a pre-trained Q-learning agent guides a new Q-learning student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUwpqZ0_SHpY"
      },
      "outputs": [],
      "source": [
        "def teacher_guided_action(state, student_q_table, teacher_q_table, epsilon,\n",
        "                         availability, accuracy, env):\n",
        "    \"\"\"Select action with teacher guidance\"\"\"\n",
        "    # Check if teacher provides advice\n",
        "    if np.random.uniform(0, 1) < availability:\n",
        "        # Teacher provides advice\n",
        "        teacher_best_action = np.argmax(teacher_q_table[state[0], state[1]])\n",
        "\n",
        "        # Check if advice is correct\n",
        "        if np.random.uniform(0, 1) < accuracy:\n",
        "            # Correct advice: use teacher's best action\n",
        "            return teacher_best_action\n",
        "        else:\n",
        "            # Incorrect advice: random action excluding teacher's best\n",
        "            possible_actions = list(range(env.action_space))\n",
        "            possible_actions.remove(teacher_best_action)\n",
        "            return np.random.choice(possible_actions)\n",
        "    else:\n",
        "        # No teacher advice: use student's epsilon-greedy policy\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            return np.random.randint(env.action_space)\n",
        "        else:\n",
        "            return np.argmax(student_q_table[state[0], state[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T0UflCdSHpY"
      },
      "outputs": [],
      "source": [
        "def train_q_learning_with_teacher(env, teacher_q_table, num_episodes,\n",
        "                                 availability, accuracy, hyperparams):\n",
        "    \"\"\"Train Q-learning with teacher guidance\"\"\"\n",
        "    # Create new student agent\n",
        "    student = QLearningAgent(env, **{k: v for k, v in hyperparams.items()\n",
        "                                    if k not in ['num_episodes', 'max_steps_per_episode', 'seed']})\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    successful_episodes = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        epsilon = student.get_epsilon(episode)\n",
        "\n",
        "        while not done and steps < hyperparams['max_steps_per_episode']:\n",
        "            # Choose action with teacher guidance\n",
        "            action = teacher_guided_action(state, student.q_table, teacher_q_table,\n",
        "                                         epsilon, availability, accuracy, env)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value\n",
        "            student.update(state, action, reward, next_state, done)\n",
        "\n",
        "            # Update state and statistics\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        # Record metrics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_steps.append(steps)\n",
        "        successful_episodes.append(1 if done and reward == 25 else 0)\n",
        "\n",
        "    return episode_rewards, episode_steps, successful_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSCjfU16SHpZ"
      },
      "outputs": [],
      "source": [
        "# Run Q-learning with teacher experiments\n",
        "print(\"Running Q-learning with teacher experiments...\")\n",
        "print(\"This will test 25 different teacher configurations.\\n\")\n",
        "\n",
        "q_teacher_results = []\n",
        "\n",
        "for avail in TEACHER_PARAMS['availability']:\n",
        "    for acc in TEACHER_PARAMS['accuracy']:\n",
        "        print(f\"Testing availability={avail}, accuracy={acc}\")\n",
        "\n",
        "        # Train with teacher\n",
        "        rewards, steps, success = train_q_learning_with_teacher(\n",
        "            env, q_agent.q_table,\n",
        "            num_episodes=300,  # Can use fewer episodes for teacher experiments\n",
        "            availability=avail,\n",
        "            accuracy=acc,\n",
        "            hyperparams=HYPERPARAMETERS\n",
        "        )\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(rewards, steps, success)\n",
        "\n",
        "        # Store results\n",
        "        q_teacher_results.append({\n",
        "            'availability': avail,\n",
        "            'accuracy': acc,\n",
        "            'avg_reward': metrics['avg_reward'],\n",
        "            'success_rate': metrics['success_rate'],\n",
        "            'avg_learning_speed': metrics['avg_learning_speed']\n",
        "        })\n",
        "\n",
        "        print(f\"  Average Reward: {metrics['avg_reward']:.2f}\")\n",
        "        print(f\"  Success Rate: {metrics['success_rate']:.1f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vro94aYSHpZ"
      },
      "outputs": [],
      "source": [
        "# Create heatmap for Q-learning with teacher\n",
        "q_teacher_df = pd.DataFrame(q_teacher_results)\n",
        "create_heatmap(q_teacher_df, \"Q-learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU_1oDNqSHpZ"
      },
      "source": [
        "# Task 4: SARSA with Teacher\n",
        "\n",
        "Implement teacher-student framework where a pre-trained SARSA agent guides a new SARSA student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IES2ZaVtSHpa"
      },
      "outputs": [],
      "source": [
        "def train_sarsa_with_teacher(env, teacher_q_table, num_episodes,\n",
        "                            availability, accuracy, hyperparams):\n",
        "    \"\"\"Train SARSA with teacher guidance\"\"\"\n",
        "    # Create new student agent\n",
        "    student = SARSAAgent(env, **{k: v for k, v in hyperparams.items()\n",
        "                                if k not in ['num_episodes', 'max_steps_per_episode', 'seed']})\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    successful_episodes = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        epsilon = student.get_epsilon(episode)\n",
        "\n",
        "        # Choose initial action with teacher guidance\n",
        "        action = teacher_guided_action(state, student.q_table, teacher_q_table,\n",
        "                                     epsilon, availability, accuracy, env)\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < hyperparams['max_steps_per_episode']:\n",
        "            # Take action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Choose next action with teacher guidance\n",
        "            next_action = teacher_guided_action(next_state, student.q_table, teacher_q_table,\n",
        "                                              epsilon, availability, accuracy, env)\n",
        "\n",
        "            # Update Q-value\n",
        "            student.update(state, action, reward, next_state, next_action, done)\n",
        "\n",
        "            # Update state and action\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        # Record metrics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_steps.append(steps)\n",
        "        successful_episodes.append(1 if done and reward == 25 else 0)\n",
        "\n",
        "    return episode_rewards, episode_steps, successful_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPyBdxySHpa"
      },
      "outputs": [],
      "source": [
        "# Run SARSA with teacher experiments\n",
        "print(\"Running SARSA with teacher experiments...\")\n",
        "print(\"This will test 25 different teacher configurations.\\n\")\n",
        "\n",
        "sarsa_teacher_results = []\n",
        "\n",
        "for avail in TEACHER_PARAMS['availability']:\n",
        "    for acc in TEACHER_PARAMS['accuracy']:\n",
        "        print(f\"Testing availability={avail}, accuracy={acc}\")\n",
        "\n",
        "        # Train with teacher\n",
        "        rewards, steps, success = train_sarsa_with_teacher(\n",
        "            env, sarsa_agent.q_table,\n",
        "            num_episodes=300,  # Can use fewer episodes for teacher experiments\n",
        "            availability=avail,\n",
        "            accuracy=acc,\n",
        "            hyperparams=HYPERPARAMETERS\n",
        "        )\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(rewards, steps, success)\n",
        "\n",
        "        # Store results\n",
        "        sarsa_teacher_results.append({\n",
        "            'availability': avail,\n",
        "            'accuracy': acc,\n",
        "            'avg_reward': metrics['avg_reward'],\n",
        "            'success_rate': metrics['success_rate'],\n",
        "            'avg_learning_speed': metrics['avg_learning_speed']\n",
        "        })\n",
        "\n",
        "        print(f\"  Average Reward: {metrics['avg_reward']:.2f}\")\n",
        "        print(f\"  Success Rate: {metrics['success_rate']:.1f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuwfEzUGSHpb"
      },
      "outputs": [],
      "source": [
        "# Create heatmap for SARSA with teacher\n",
        "sarsa_teacher_df = pd.DataFrame(sarsa_teacher_results)\n",
        "create_heatmap(sarsa_teacher_df, \"SARSA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ4XEvm-SHpb"
      },
      "source": [
        "# Analysis and Discussion\n",
        "\n",
        "## Teacher Impact Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz02-fvGSHpb"
      },
      "outputs": [],
      "source": [
        "# Analyze teacher impact for specific availability levels\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Q-learning analysis\n",
        "for avail in [0.1, 0.5, 1.0]:\n",
        "    df_subset = q_teacher_df[q_teacher_df['availability'] == avail]\n",
        "    ax1.plot(df_subset['accuracy'], df_subset['avg_reward'],\n",
        "             marker='o', label=f'Availability={avail}')\n",
        "\n",
        "ax1.axhline(y=q_metrics['avg_reward'], color='red', linestyle='--',\n",
        "           label='Baseline (no teacher)')\n",
        "ax1.set_xlabel('Teacher Accuracy')\n",
        "ax1.set_ylabel('Average Reward')\n",
        "ax1.set_title('Q-learning: Teacher Impact Analysis')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# SARSA analysis\n",
        "for avail in [0.1, 0.5, 1.0]:\n",
        "    df_subset = sarsa_teacher_df[sarsa_teacher_df['availability'] == avail]\n",
        "    ax2.plot(df_subset['accuracy'], df_subset['avg_reward'],\n",
        "             marker='o', label=f'Availability={avail}')\n",
        "\n",
        "ax2.axhline(y=sarsa_metrics['avg_reward'], color='red', linestyle='--',\n",
        "           label='Baseline (no teacher)')\n",
        "ax2.set_xlabel('Teacher Accuracy')\n",
        "ax2.set_ylabel('Average Reward')\n",
        "ax2.set_title('SARSA: Teacher Impact Analysis')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0N5xiRSHpb"
      },
      "source": [
        "## Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48L-kXp9SHpc"
      },
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "summary_data = {\n",
        "    'Algorithm': ['Q-learning (baseline)', 'SARSA (baseline)',\n",
        "                  'Q-learning (best teacher)', 'SARSA (best teacher)'],\n",
        "    'Success Rate (%)': [\n",
        "        q_metrics['success_rate'],\n",
        "        sarsa_metrics['success_rate'],\n",
        "        q_teacher_df.loc[q_teacher_df['avg_reward'].idxmax(), 'success_rate'],\n",
        "        sarsa_teacher_df.loc[sarsa_teacher_df['avg_reward'].idxmax(), 'success_rate']\n",
        "    ],\n",
        "    'Average Reward': [\n",
        "        q_metrics['avg_reward'],\n",
        "        sarsa_metrics['avg_reward'],\n",
        "        q_teacher_df['avg_reward'].max(),\n",
        "        sarsa_teacher_df['avg_reward'].max()\n",
        "    ],\n",
        "    'Learning Speed': [\n",
        "        q_metrics['avg_learning_speed'],\n",
        "        sarsa_metrics['avg_learning_speed'],\n",
        "        q_teacher_df.loc[q_teacher_df['avg_reward'].idxmax(), 'avg_learning_speed'],\n",
        "        sarsa_teacher_df.loc[sarsa_teacher_df['avg_reward'].idxmax(), 'avg_learning_speed']\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\nPerformance Summary:\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Find best teacher configurations\n",
        "best_q_config = q_teacher_df.loc[q_teacher_df['avg_reward'].idxmax()]\n",
        "best_sarsa_config = sarsa_teacher_df.loc[sarsa_teacher_df['avg_reward'].idxmax()]\n",
        "\n",
        "print(\"\\nBest Teacher Configurations:\")\n",
        "print(f\"Q-learning: Availability={best_q_config['availability']}, \"\n",
        "      f\"Accuracy={best_q_config['accuracy']}\")\n",
        "print(f\"SARSA: Availability={best_sarsa_config['availability']}, \"\n",
        "      f\"Accuracy={best_sarsa_config['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBCjVt54SHpc"
      },
      "source": [
        "## Key Findings\n",
        "\n",
        "### 1. **Algorithm Comparison (Baseline Performance)**\n",
        "\n",
        "From Figures 1-3 and the summary table:\n",
        "\n",
        "**Q-learning vs SARSA baseline performance:**\n",
        "- Q-learning achieved 88.2% success rate with average reward of -11.21\n",
        "- SARSA achieved 86.6% success rate with average reward of -12.80\n",
        "- Q-learning slightly outperforms SARSA (approximately 1.6% difference in success rate)\n",
        "\n",
        "**Analysis:**\n",
        "Q-learning's advantage stems from its off-policy nature. It always updates based on the maximum Q-value of the next state (greedy assumption), enabling faster identification of optimal paths during exploration. SARSA, being an on-policy algorithm, considers the actual action to be taken (including exploratory actions), resulting in more conservative learning and slightly slower convergence.\n",
        "\n",
        "### 2. **Teacher Impact Analysis**\n",
        "\n",
        "From the heatmaps (Figures 4-5), several key observations emerge:\n",
        "\n",
        "**Negative impact zones:**\n",
        "- When accuracy ≤ 0.3, teacher guidance is generally harmful\n",
        "- Worst case scenario: high availability (1.0) + low accuracy (0.1)\n",
        "  - Q-learning: -152.3 (141 points worse than baseline)\n",
        "  - SARSA: -198.5 (186 points worse than baseline)\n",
        "\n",
        "**Positive impact zones:**\n",
        "- When accuracy ≥ 0.7, teacher guidance becomes significantly beneficial\n",
        "- Best performance occurs in high accuracy regions\n",
        "\n",
        "**Critical point analysis:**\n",
        "Figure 6 reveals that accuracy ≈ 0.5 is a critical threshold. Below this value, teacher guidance is detrimental; above it, positive effects emerge.\n",
        "\n",
        "### 3. **Optimal Teacher Configuration**\n",
        "\n",
        "**Q-learning optimal configuration:**\n",
        "- Availability = 1.0, Accuracy = 1.0\n",
        "- Average reward: 13.38 (improvement of 24.59 from baseline)\n",
        "- Success rate: 98.0% (9.8% improvement)\n",
        "\n",
        "**SARSA optimal configuration:**\n",
        "- Availability = 1.0, Accuracy = 1.0\n",
        "- Average reward: 14.23 (improvement of 27.03 from baseline)\n",
        "- Success rate: 98.0% (11.4% improvement)\n",
        "\n",
        "**Interesting finding:**\n",
        "Counter-intuitively, for Q-learning with accuracy=1.0, availability=0.1 (13.4) performs almost identically to availability=1.0 (13.38). This suggests that for perfect teachers, occasional correct guidance is sufficient.\n",
        "\n",
        "### 4. **Algorithm Sensitivity Analysis**\n",
        "\n",
        "**SARSA is more sensitive to teacher guidance:**\n",
        "\n",
        "1. **Negative sensitivity:**\n",
        "   - SARSA performs worse under low-accuracy teachers (-198.5 vs -152.3)\n",
        "   - Indicates that incorrect guidance is more destructive to conservative algorithms\n",
        "\n",
        "2. **Positive sensitivity:**\n",
        "   - SARSA benefits more from good teachers (improvement of 27.03 vs 24.59)\n",
        "   - In high accuracy regions, SARSA's improvement curve is steeper (Figure 6, right)\n",
        "\n",
        "**Explanation:**\n",
        "SARSA's on-policy nature makes it more dependent on current policy quality. Good teacher guidance directly improves policy quality, creating a compound effect. Q-learning's off-policy nature provides some \"interference resistance,\" making it less sensitive to variations in teacher quality.\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "### Main Conclusions:\n",
        "\n",
        "1. **Baseline performance:** Q-learning slightly outperforms conservative SARSA without teacher guidance due to its aggressive learning strategy.\n",
        "\n",
        "2. **Teacher quality threshold:** A clear quality threshold exists (accuracy ≈ 0.5). Teacher guidance below this threshold is harmful, emphasizing the importance of validating expert knowledge quality in practical applications.\n",
        "\n",
        "3. **Optimal guidance strategy:** High-quality continuous guidance (high accuracy + high availability) is most effective for both algorithms. Interestingly, occasional guidance from a perfect teacher can achieve similar results.\n",
        "\n",
        "4. **Algorithm selection recommendations:**\n",
        "   - No teacher/uncertain teacher quality: Choose Q-learning (more robust)\n",
        "   - High-quality teacher available: Choose SARSA (greater benefits)\n",
        "\n",
        "### Practical Implications:\n",
        "\n",
        "This research provides important insights for real-world reinforcement learning applications:\n",
        "- Expert accuracy must be evaluated before incorporating human guidance\n",
        "- Low-quality \"expert\" guidance is worse than allowing agents to learn autonomously\n",
        "- Different RL algorithms respond differently to external guidance, requiring appropriate algorithm selection based on available resources"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}